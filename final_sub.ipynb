{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5e7D00P9Ztn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46e02fe3-4d7e-4aa0-9404-1ef2edc9051b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O01C2cZy-BnB",
        "colab_type": "text"
      },
      "source": [
        "**Imports and functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I0qAjJvpSn_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "846ad89a-5320-4a44-cb1e-95e16f5af475"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import warnings \n",
        "import datetime\n",
        "import pickle\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rIE3PRwfho9m",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "  #paste the kaggle kernel link\n",
        "  '''\n",
        "  The data size is too big to get rid of memory error this method will reduce memory\n",
        "  usage by changing types. It does the following\n",
        "  Load objects as categories\n",
        "  Binary values are switched to int8\n",
        "  Binary values with missing values are switched to float16\n",
        "  64 bits encoding are all switched to 32 or 16bits if possible.\n",
        "  Parameters\n",
        "  ---------------\n",
        "  df - DataFrame whose size to be reduced\n",
        "  ---------------\n",
        "  '''\n",
        "\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "  start_mem = df.memory_usage().sum() / 1024**2    \n",
        "  for col in df.columns:\n",
        "      col_type = df[col].dtypes\n",
        "      if col_type in numerics:\n",
        "          c_min = df[col].min()\n",
        "          c_max = df[col].max()\n",
        "          if str(col_type)[:3] == 'int':\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                  df[col] = df[col].astype(np.int8)\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                  df[col] = df[col].astype(np.int16)\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                  df[col] = df[col].astype(np.int32)\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                  df[col] = df[col].astype(np.int64)  \n",
        "          else:\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                  df[col] = df[col].astype(np.float16)\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                  df[col] = df[col].astype(np.float32)\n",
        "              else:\n",
        "                  df[col] = df[col].astype(np.float64)    \n",
        "  end_mem = df.memory_usage().sum() / 1024**2\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "  return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "geNDzSaVg9ZR",
        "colab": {}
      },
      "source": [
        "def get_basic_time_feat(df, grpby, col, s):\n",
        "  '''\n",
        "  create basic time feats like differece in minute, days etcetera\n",
        "  and return the dataframe.\n",
        "  \n",
        "  Parameters\n",
        "  ---------------------\n",
        "  df      - Features will be created\n",
        "  grpby   - group the DF based on this value\n",
        "  col     - column where the operations will be performed\n",
        "  s       - shift value\n",
        "  ---------------------\n",
        "  '''\n",
        "\n",
        "  df = df.sort_values(col)\n",
        "  for i in range(s):\n",
        "    df['prev_{}_'.format(i+1)+col] = df.groupby([grpby])[col].shift(i+1)\n",
        "    df['purchase_date_diff_{}_days'.format(i+1)] = (df[col] - df['prev_{}_'.format(i+1)+col]).dt.days.values\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] = df['purchase_date_diff_{}_days'.format(i+1)].values * 24 * 3600\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] += (df[col] - df['prev_{}_'.format(i+1)+col]).dt.seconds.values\n",
        "    df['purchase_date_diff_{}_hours'.format(i+1)] = df.iloc[:, -1].values // 3600\n",
        "\n",
        "  return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IXvviVuhhJgl",
        "colab": {}
      },
      "source": [
        "def s_agg(new_df, df, op, prefix, grpby, col):\n",
        "  '''\n",
        "  takes the data frame as input and return the dataframe with the aggregate operations performed.\n",
        "  \n",
        "  Parameters\n",
        "  ----------------------------\n",
        "  new_df  - DF with new features added\n",
        "  df      - original DF\n",
        "  op      - statistical operations like min, max, mean etc.\n",
        "  prefix  - prefix for the feature name\n",
        "  grpby   - based on which column to group by\n",
        "  col     - operations will be performed on this column\n",
        "  ----------------------------\n",
        "  '''\n",
        "\n",
        "  for o in op:\n",
        "    new_df[prefix+col+'_{}'.format(o)] = df.groupby([grpby])[col].agg([o]).values\n",
        "  return new_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rrNKumPOhOGu",
        "colab": {}
      },
      "source": [
        "def find_single_val(new_df, df, col, grpby, op, name='',  prefix='', use_col=False):\n",
        "  '''\n",
        "  find a value like min, max, mean in the specified column and return the DF\n",
        "  \n",
        "  Parameters\n",
        "  ------------------\n",
        "  new_df   - features will be added to this DF\n",
        "  df       - original DF from which the features will be created\n",
        "  col      - operations will be performed on this column\n",
        "  grpby    - based on this column we'll to group by\n",
        "  name     - name for the new features created\n",
        "  op       - statistical operations to be performed\n",
        "  prefix   - added to the name of the feature -- default value empty\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\n",
        "  ------------------\n",
        "  '''\n",
        "  \n",
        "  if use_col:\n",
        "    for c in col:\n",
        "      for o in op:\n",
        "        if o is 'min':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].min().values\n",
        "        elif o is 'max':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].max().values\n",
        "        elif o is 'mean':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].mean().values\n",
        "        elif o is 'sum':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].sum().values\n",
        "        elif o is 'nunique':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].nunique().values\n",
        "        elif o is 'std':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].std().values\n",
        "        elif o is 'count':\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].count().values\n",
        "\n",
        "  else:\n",
        "    for c in col:\n",
        "      for o in op:\n",
        "        if o is 'min':\n",
        "          new_df[name] = df.groupby([grpby])[c].min().values\n",
        "        elif o is 'max':\n",
        "          new_df[name] = df.groupby([grpby])[c].max().values\n",
        "        elif o is 'mean':\n",
        "          new_df[name] = df.groupby([grpby])[c].mean().values\n",
        "        elif o is 'sum':\n",
        "          new_df[name] = df.groupby([grpby])[c].sum().values\n",
        "        elif o is 'nunique':\n",
        "          new_df[name] = df.groupby([grpby])[c].nunique().values\n",
        "        elif o is 'std':\n",
        "          new_df[name] = df.groupby([grpby])[c].std().values\n",
        "        elif o is 'count':\n",
        "          new_df[name] = df.groupby([grpby])[c].count().values\n",
        "\n",
        "  return new_df "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p_Y1Bwtbizl9",
        "colab": {}
      },
      "source": [
        "def get_monthlag_stat(new_df, df, grpby, op, col, name, prefix=''):\n",
        "  \n",
        "  '''\n",
        "  group by the the specified column and find the count or sum depending on the input.\n",
        "  Then perform basic operations like std, min, max etcetera\n",
        "  parameters\n",
        "  ----------------------------\n",
        "  new_df - new features will be added to this DF\n",
        "  df     - original DF\n",
        "  grpby  - column using which we will group the data by\n",
        "  col    - operations will be performed on this column\n",
        "  name   - name for this columnn\n",
        "  prefix - prefix to the column name\n",
        "  ----------------------------\n",
        "  '''\n",
        "  if op == 'sum':\n",
        "    tmp = df.groupby(grpby)[col].sum().unstack()\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, -1].values\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, -2].values\n",
        "  \n",
        "  if op == 'count':\n",
        "    tmp = df.groupby(grpby)[col].count().unstack()\n",
        "    # check if there is any null value and fill it with 0\n",
        "    # for the sum we are not performing any null value imputation\n",
        "    # as we are directly using the value. However, here we are performing operations like\n",
        "    # min, max, std etcetera so we are imputing the null values.\n",
        "    if tmp.isna().sum().any() > 0:\n",
        "      tmp = tmp.fillna(0.0)\n",
        "    new_df[prefix+grpby[1]+'_'+name[0]] = tmp.reset_index().iloc[:, 1:].std(axis=1).values\n",
        "    new_df[prefix+grpby[1]+'_'+name[1]] = tmp.reset_index().iloc[:, 1:].max(axis=1).values\n",
        "  return new_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tvXEO5yWngKp",
        "colab": {}
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world?scriptVersionId=8335387\n",
        "def successive_aggregates(df, field1, field2):\n",
        "    '''\n",
        "    what this function does is that it group the data twice and find\n",
        "    basic aggregate values.\n",
        "    First it will goup by card_id and all the specified column one by one.\n",
        "    Then it will find the agg values like mean, min, max and std\n",
        "    for the purchase amount for each group.\n",
        "    Parameters\n",
        "    -------------------\n",
        "    df      - original DataFrame\n",
        "    field1  - first groupby along with card_id\n",
        "    field2  - second grouby along with card_id\n",
        "    -------------------\n",
        "    '''\n",
        "\n",
        "    t = df.groupby(['card_id', field1])[field2].mean()\n",
        "    u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\n",
        "    u.columns = ['new_transac_' + field1 + '_' + field2 + '_' + col for col in u.columns.values]\n",
        "    u.reset_index(inplace=True)\n",
        "    return u"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AEun1HZzEcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_influential(df, col_name, date):\n",
        "  '''\n",
        "  This function return whether a purchase is influential or not.\n",
        "  A purchase is considered influential if it is made 100 days before a festival.\n",
        "  If it is not influential it will give a value 0 else the actual value.\n",
        "  Parameters\n",
        "  --------------------------------\n",
        "  df       - Dataframe where the operations will be performed\n",
        "  col_name - name of the new feature\n",
        "  date     - on which date the holiday is occuring\n",
        "  --------------------------------\n",
        "  '''\n",
        "\n",
        "  df[col_name] = (pd.to_datetime(date) - pd.to_datetime(df['purchase_date'])).dt.days\n",
        "  df[col_name] = df[col_name].apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HunjDtpm7Tmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lab_enc_load(df, col, file):\n",
        "  '''\n",
        "  This function will laod the saved label encoder and\n",
        "  transform the data.\n",
        "\n",
        "  Parameter\n",
        "  ------------------------\n",
        "  df   - Return the dataframe after encoded\n",
        "  col  - Column in which the encoding will be done\n",
        "  file - location of the label encoder\n",
        "  ------------------------\n",
        "  '''\n",
        "  lbl = LabelEncoder()\n",
        "  lbl.classes_ = np.load(file, allow_pickle=True)\n",
        "  df[col] = lbl.transform(df[col].astype(str))\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLPBuzq897CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "  print('Preprocessing New Merchant Dataset...')\n",
        "  new_merchant = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])\n",
        "  new_merchant = reduce_mem_usage(new_merchant)\n",
        "\n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/new_merch_fill_na.csv'):\n",
        "        print('Filled Missing values for new_merchant...')\n",
        "        del new_merchant;gc.collect()\n",
        "  else:\n",
        "    \n",
        "    a = pd.DataFrame()\n",
        "    a['card_id'] = new_merchant['card_id']\n",
        "    a['merchant_id'] = new_merchant['merchant_id']\n",
        "    a['purchase_date'] = new_merchant['purchase_date']\n",
        "\n",
        "    new_merchant.drop(['card_id', 'merchant_id', 'purchase_date'], axis=1, inplace=True)\n",
        "    gc.collect()\n",
        "\n",
        "    feat = new_merchant.columns\n",
        "    cols = ['category_2', 'category_3']\n",
        "    #laabel encode the variables\n",
        "    \n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/new_merchant_authorized_flag_enc.npy'\n",
        "    new_merchant['authorized_flag'] = lab_enc_load(new_merchant, 'authorized_flag', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/new_merchant_category_1_enc.npy'\n",
        "    new_merchant['category_1'] = lab_enc_load(new_merchant, 'category_1', file)\n",
        "\n",
        "    #list to hold the null values\n",
        "    no_nan = []\n",
        "    #select only columns which doesn't have any null values\n",
        "    for c in feat:\n",
        "      if c not in cols:\n",
        "        no_nan.append(c)\n",
        "   \n",
        "    #label encode the category 3 variables before predicting\n",
        "    d = {'A':1, 'B':2, 'C':3}\n",
        "    test['category_3'] = test['category_3'].map(d)\n",
        "\n",
        "    #Loading the model and pedicing the category_2 misssing\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/clf_cat2.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    #create a test set by selecting only rows which are having null values\n",
        "    test = new_merchant[new_merchant['category_2'].isna()]\n",
        "    #make prediction only for the rows with null value\n",
        "    new_merchant.loc[new_merchant['category_2'].isna(), 'category_2'] = mod.predict(test[no_nan])\n",
        "\n",
        "    #Loading the model and pedicing the category_3 misssing\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/clf_cat3.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = new_merchant[new_merchant['category_3'].isna()]\n",
        "    new_merchant.loc[new_merchant['category_3'].isna(), 'category_3'] = mod.predict(test[no_nan])\n",
        "\n",
        "    new_merchant['card_id'] = a['card_id']\n",
        "    new_merchant['merchant_id'] = a['merchant_id']\n",
        "    new_merchant['purchase_date'] = a['purchase_date']\n",
        "    del a, new_merchant, new_merch_fill_na ;gc.collect()\n",
        "    print('Saving the file...')\n",
        "    new_merchant.to_csv('tnew_merch_fill_na.csv')\n",
        "    !cp tnew_merch_fill_na.csv \"/content/drive/My Drive/case study/upload 15mis/\"\n",
        "    print('Filled missing values for new_merchant...')\n",
        "\n",
        "  print('Working on Historical Transaction Dataset...')\n",
        "  \n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/ht_fill_na.csv'):\n",
        "    print('Filled missing values for historical transaction dataset...')\n",
        "    #del ht_fill_na;gc.collect()\n",
        "  else:\n",
        "    a = pd.DataFrame()\n",
        "    a['card_id'] = ht['card_id']\n",
        "    a['merchant_id'] = ht['merchant_id']\n",
        "    a['purchase_date'] = ht['purchase_date']\n",
        "\n",
        "    ht.drop(['card_id', 'merchant_id', 'purchase_date'], axis=1, inplace=True)\n",
        "    gc.collect()\n",
        "\n",
        "    feat = ht.columns\n",
        "    cols = ['category_2', 'category_3']\n",
        "    #laabel encode the variables\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/ht_authorized_flag_enc.npy'\n",
        "    ht['authoirzed_flag'] = lab_enc_load(ht, 'authorized_flag', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/ht_category_1_enc.npy'\n",
        "    ht['category_1'] =lab_enc_load(ht, 'category_1', file)\n",
        "\n",
        "    #list to hold the null values\n",
        "    no_nan = []\n",
        "    #select only columns which doesn't have any null values\n",
        "    for c in feat:\n",
        "      if c not in cols:\n",
        "        no_nan.append(c)\n",
        "\n",
        "    #label encode the category 3 variables before feeding it to the model\n",
        "    d = {'A':1, 'B':2, 'C':3}\n",
        "    test['category_3'] = test['category_3'].map(d)\n",
        "\n",
        "    #Loading the model and pedicing the category_2 misssing\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/ht_clf_cat2.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = ht[ht['category_2'].isna()]\n",
        "    #make prediction only for the rows with null value\n",
        "    ht.loc[ht['category_2'].isna(), 'category_2'] = mod.predict(test[no_nan])\n",
        "    \n",
        "    #Loading the model and pedicing the category_3 misssing\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/ht_clf_cat3.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = ht[ht['category_3'].isna()]\n",
        "    ht.loc[ht['category_3'].isna(), 'category_3'] = mod.predict(test[no_nan])\n",
        "\n",
        "    ht['card_id'] = a['card_id']\n",
        "    ht['merchant_id'] = a['merchant_id']\n",
        "    ht['purchase_date'] = a['purchase_date']\n",
        "    \n",
        "    print('Saving the file...')\n",
        "    ht.to_csv('tht_fill_na.csv')\n",
        "    !cp tht_fill_na.csv \"/content/drive/My Drive/case study/upload 15mis/\"\n",
        "    print('Filled missing values for historical transactions...')\n",
        "    del ht,a;gc.collect()\n",
        "\n",
        "  print('Loading Merchant Dataset for preprocessing...')\n",
        "  merchant = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/merchants.csv')\n",
        "  merchant = reduce_mem_usage(merchant)\n",
        "  \n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/merchants.csv'):\n",
        "    #processed = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/merchants.csv')\n",
        "    print('Filled missing values for merchant dataset...')\n",
        "  else:\n",
        "    merchant = merchant[merchant['avg_purchases_lag3']!=np.inf]\n",
        "    tmp = pd.DataFrame()\n",
        "    tmp['merchant_id'] = merchant['merchant_id']\n",
        "    tmp['category_2'] = merchant['category_2']\n",
        "\n",
        "    merchant.drop(['merchant_id', 'category_2'], axis=1, inplace=True)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/merchant_category_4_enc.npy'\n",
        "    merchant['category_4'] = lab_enc_load(merchant, 'category_4', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/merchant_category_1_enc.npy'\n",
        "    merchant['category_1'] = lab_enc_load(merchant, 'category_1', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/merchant_most_recent_sales_range_enc.npy'\n",
        "    merchant['most_recent_sales_range'] = lab_enc_load(merchant, 'most_recent_sales_range', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/merchant_most_recent_purchases_range_enc.npy'\n",
        "    merchant['most_recent_purchases_range'] = lab_enc_load(merchant, 'most_recent_purchases_range', file)\n",
        "\n",
        "    feat = merchant.columns\n",
        "    cols = ['avg_sales_lag3','avg_sales_lag6','avg_sales_lag12']\n",
        "    no_nan = []\n",
        "\n",
        "    for c in feat:\n",
        "      if c not in cols:\n",
        "        no_nan.append(c)\n",
        "\n",
        "    #Loading the model and predict the missing values in avg_sales_lag3\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/merch_clf_knn.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = merchant[merchant['avg_sales_lag3'].isna()]\n",
        "    merchant.loc[merchant['avg_sales_lag3'].isna(), 'avg_sales_lag3'] = mod.predict(test[no_nan])\n",
        "\n",
        "    #Loading the model and predict the missing values in avg_sales_lag6\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/merch_clf2_knn.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = merchant[merchant['avg_sales_lag6'].isna()]\n",
        "    merchant.loc[merchant['avg_sales_lag6'].isna(), 'avg_sales_lag6'] = mod.predict(test[no_nan])\n",
        "\n",
        "    #Loading the model and predict the missing values in avg_sales_lag12\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/merch_clf3_knn.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    test = merchant[merchant['avg_sales_lag12'].isna()]\n",
        "    merchant.loc[merchant['avg_sales_lag12'].isna(), 'avg_sales_lag12'] = mod.predict(test[no_nan])\n",
        "\n",
        "    #predicting the missing value for category_2\n",
        "    merchant['category_2'] = tmp['category_2']\n",
        "\n",
        "    feat = merchant.columns\n",
        "    cols = ['category_2']\n",
        "    no_nan = []\n",
        "\n",
        "    for c in feat:\n",
        "      if c not in cols:\n",
        "        no_nan.append(c)\n",
        "\n",
        "    test = merchant[merchant['category_2'].isna()]\n",
        "    with open('/content/drive/My Drive/case study/upload 15mis/merch_clf_cat2.sav', 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "    merchant.loc[merchant['category_2'].isna(), 'category_2'] = mod.predict(test[no_nan])\n",
        "\n",
        "    merchant['merchant_id'] = tmp['merchant_id']\n",
        "    merchant.to_csv('tmerch_fill_na.csv')\n",
        "    !cp tmerch_fill_na.csv \"/content/drive/My Drive/case study/upload 15mis/\"\n",
        "    del merchant;gc.collect()\n",
        "  \n",
        "  #print('Loading the dataset filled NaN...')\n",
        "  \n",
        "  print('One Hot Encoding the variables...')\n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/new_merchant_processed_fill_na.csv'):\n",
        "    print('Completed...')\n",
        "    #del new_merch_fill_na;gc.collect()\n",
        "\n",
        "  else:\n",
        "    new_merchant = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merch_fill_na.csv')\n",
        "    new_merchant = reduce_mem_usage(new_merchant)\n",
        "    ht = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/ht_fill_na.csv')\n",
        "    ht = reduce_mem_usage(ht)\n",
        "    \n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/ht_category_3_enc.npy'\n",
        "    ht['category_3'] = lab_enc_load(ht, 'category_3', file)\n",
        "\n",
        "    file = '/content/drive/My Drive/case study/upload 15mis/label/new_merchant_category_3_enc.npy'\n",
        "    new_merchant['category_3'] = lab_enc_load(new_merchant, 'category_3', file)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    mont = [0,-1,-2,-3,-4,-5,-6]\n",
        "    cat_2 = [1.,2.,3.,4.,5.]\n",
        "    cat_3 = [0,1,2,3]\n",
        "\n",
        "    for val in mont:\n",
        "      ht['month_lag={}'.format(val)] = (ht['month_lag'] == val).astype(int)\n",
        "\n",
        "    for val in cat_2:\n",
        "      ht['category_2={}'.format(int(val))] = (ht['category_2'] == val).astype(int)\n",
        "\n",
        "    for val in cat_3:\n",
        "      ht['category_3={}'.format(int(val))] = (ht['category_3'] == val).astype(int)\n",
        "    gc.collect()\n",
        "\n",
        "    cat_2 = [1.,2.,3.,4.,5.]\n",
        "    cat_3 = [0,1,2,3]\n",
        "    mont = [1,2]\n",
        "\n",
        "    for val in mont:\n",
        "      new_merchant['month_lag={}'.format(val)] = (new_merchant['month_lag'] == val).astype(int)\n",
        "    for val in cat_2:\n",
        "      new_merchant['category_2={}'.format(int(val))] = (new_merchant['category_2'] == val).astype(int)\n",
        "    for val in cat_3:\n",
        "      new_merchant['category_3={}'.format(int(val))] = (new_merchant['category_3'] == val).astype(int)\n",
        "    gc.collect()\n",
        "\n",
        "    ht['purchase_month'] = ht['purchase_date'].astype(str)\n",
        "    ht['reference_month'] = pd.to_datetime(ht['purchase_month'].apply(lambda x: x[:7] + '-28')) - \\\n",
        "                                          ht['month_lag'].apply(lambda x: np.timedelta64(x, 'M'))\n",
        "    gc.collect()\n",
        "\n",
        "    ht['reference_month'] = [x[:7] for x in ht['reference_month'].astype(str)]\n",
        "    del ht['purchase_month'];gc.collect()\n",
        "\n",
        "    new_merchant['reference_month'] = (pd.to_datetime(pd.DatetimeIndex(new_merchant['purchase_date']).date) - \\\n",
        "                                   new_merchant['month_lag'].apply(lambda x: np.timedelta64(x, 'M')))\n",
        "    new_merchant['reference_month'] = [x[:7] for x in new_merchant['reference_month'].astype(str)]\n",
        "\n",
        "    new_merchant.to_csv('tnew_merchant_processed_fill_na.csv', index=False)\n",
        "    !cp tnew_merchant_processed.csv \"/content/drive/My Drive/Colab Notebooks/ELO\"\n",
        "    ht.to_csv('tht_processed_fill_na.csv', index=False)\n",
        "    !cp tht_processed.csv \"/content/drive/My Drive/Colab Notebooks/ELO\"\n",
        "\n",
        "    #del new_merch_fill_na;gc.collect()\n",
        "\n",
        "    print('Completed One Hot Encoding...')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vt8PF_ZTR5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fe_inf():\n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/new_merch_info_fillna.csv'):\n",
        "    print('Completed FE of transaction info...')\n",
        "  \n",
        "  else:\n",
        "    new = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merch_fill_na_processed.csv')\n",
        "    new_merchant_feats = pd.DataFrame(new.groupby(['card_id']).size()).reset_index()\n",
        "    new_merchant_feats.columns = ['card_id', 'new_transac_count']\n",
        "    new['purchase_amount'] = np.round(new['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "    ht = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/ht_processed_fill_na.csv')\n",
        "    historical_trans_features = pd.DataFrame(ht.groupby(['card_id']).size()).reset_index()\n",
        "    historical_trans_features.columns = ['card_id', 'hist_transac_count']\n",
        "    ht['purchase_amount'] = np.round(ht['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "    cols = ['city_id', 'state_id', 'merchant_category_id', 'subsector_id', 'merchant_id']\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=cols, grpby='card_id',\\\n",
        "                                op=['nunique'], prefix='new_transac', use_col=True)\n",
        "\n",
        "    cols = ['city_id', 'state_id', 'merchant_category_id', 'subsector_id', 'merchant_id']\n",
        "    historical_trans_features = find_single_val(historical_trans_features, new, col=cols, grpby='card_id',\\\n",
        "                              op=['nunique'], prefix='hist_transac', use_col=True)\n",
        "    \n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['category_1'], grpby='card_id',\\\n",
        "                                op=['sum'], prefix='new_transac', use_col=True)\n",
        "    new_merchant_feats['new_transac_category_0_sum'] = new_merchant_feats['new_transac_count'].values - new_merchant_feats.iloc[:, -1].values\n",
        "\n",
        "    historical_trans_features = find_single_val(historical_trans_features, new, col=['category_1'], grpby='card_id',\\\n",
        "                                op=['sum'], prefix='hist_transac', use_col=True)\n",
        "    historical_trans_features['hist_transac_category_0_sum'] = historical_trans_features['hist_transac_count'].values - \\\n",
        "                                                            historical_trans_features.iloc[:, -1].values\n",
        "\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['category_1'], grpby='card_id',\\\n",
        "                                op=['mean','std'], prefix='new_transac', use_col=True)\n",
        "\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, col='installments', grpby='card_id', \\\n",
        "                              op=['mean', 'sum', 'max', 'min', 'std', 'skew'], prefix='new_transac_')\n",
        "\n",
        "    historical_trans_features = find_single_val(historical_trans_features, new, col=['category_1'], grpby='card_id',\\\n",
        "                                                op=['mean','std'], prefix='hist_transac', use_col=True)\n",
        "\n",
        "    historical_trans_features = s_agg(historical_trans_features, new, col='installments', grpby='card_id', \\\n",
        "                                      op=['mean', 'sum', 'max', 'min', 'std', 'skew'], \\\n",
        "                      prefix='hist_transac')\n",
        "    \n",
        "    cols = ['category_2=1', 'category_2=2', 'category_2=3', 'category_2=4', 'category_2=5',\n",
        "            'category_3=0', 'category_3=1', 'category_3=2', 'category_3=3']\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=cols, grpby='card_id',\\\n",
        "                                        op=['mean','sum'], prefix='new_transac', use_col=True)\n",
        "\n",
        "    cols = ['category_2=1', 'category_2=2', 'category_2=3', 'category_2=4', 'category_2=5',\n",
        "            'category_3=0', 'category_3=1', 'category_3=2', 'category_3=3']\n",
        "    historical_trans_features = find_single_val(historical_trans_features, new, col=cols, grpby='card_id',\\\n",
        "                                                op=['mean','sum'], prefix='new_transac', use_col=True)\n",
        "    \n",
        "    historical_trans_features = get_monthlag_stat(historical_trans_features, new, grpby=['card_id','month_lag'], op='count', \\\n",
        "                                              col='purchase_amount', prefix='hist_transac', name=['count_std','count_max'])\n",
        "\n",
        "    historical_trans_features = find_single_val(historical_trans_features, new, col=['authorized_flag'], grpby='card_id',\\\n",
        "                                                op=['sum', 'mean'], prefix='hist_transac', use_col=True)\n",
        "    historical_trans_features['hist_transac_denied_count'] = historical_trans_features['hist_transac_count'].values - \\\n",
        "                                                            historical_trans_features.iloc[:, -1].values\n",
        "\n",
        "    historical_trans_features['hist_transac_merchant_id_count_mean'] = historical_trans_features['hist_transac_count'].values \\\n",
        "                                                                    / historical_trans_features['hist_transac_merchant_id_nunique'].values\n",
        "\n",
        "    historical_trans_features['hist_transac_merchant_count_max'] = ht.groupby(['card_id', 'merchant_id']).size().reset_index().\\\n",
        "                                                                  groupby(['card_id'])[0].max().values\n",
        "\n",
        "    new_merchant_feats = get_monthlag_stat(new_merchant_feats, new, grpby=['card_id','month_lag'], op='count', \\\n",
        "                                          col='purchase_amount', prefix='new_transac_', name=['count_std','count_max'])\n",
        "    \n",
        "    historical_trans_features['hist_transac_merchant_ratio'] = historical_trans_features.iloc[:, -1].values \\\n",
        "                                                                          / historical_trans_features['hist_transac_count'].values\n",
        "    historical_trans_features['hist_transac_merchant_id_ratio'] = historical_trans_features.iloc[:, -2].values \\\n",
        "                                                                              / historical_trans_features['hist_transac_merchant_id_count_mean'].values\n",
        "    historical_trans_features['hist_transac_merchant_count_std'] = ht.groupby(['card_id', 'merchant_id']).size().reset_index().\\\n",
        "                                                                  groupby(['card_id'])[0].std().values\n",
        "    historical_trans_features.to_csv('thist_transac_info_fill_na.csv')\n",
        "    new_merchant_feats.to_csv('tnew_merch_info_fillna.csv')\n",
        "    print('Completed FE of transaction info...')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39jMDzwB-Hs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fe_am():\n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/new_merch_amount_fillna.csv'):\n",
        "    print('Completed FE of purchase amount...')\n",
        "  \n",
        "  else:\n",
        "    new = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merchant_processed_fill_na.csv')\n",
        "    new_merchant_feats = pd.DataFrame(new.groupby(['card_id']).size()).reset_index()\n",
        "    new_merchant_feats.columns = ['card_id', 'new_transac_count']\n",
        "    #the purchase amount given to us is normalized. It does not make any sense if we look at it.\n",
        "    #Credits to the user radar he somehow deanonymize the data and give the below formula to transform the purchase\n",
        "    #amount which will make much sense\n",
        "    # kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights \n",
        "    new['purchase_amount'] = np.round(new['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "    ht = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/ht_processed_fill_na.csv')\n",
        "    historical_trans_features = pd.DataFrame(ht.groupby(['card_id']).size()).reset_index()\n",
        "    historical_trans_features.columns = ['card_id', 'hist_transac_count']\n",
        "    ht['purchase_amount'] = np.round(ht['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "    #crete agg features based on the purchase amount \n",
        "    op = ['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=op, prefix='new_transac_', col='purchase_amount', grpby='card_id')\n",
        "    #finding the difference between the maximum and minmum purchase amount\n",
        "    new_merchant_feats['new_transac_amount_diff'] = new_merchant_feats['new_transac_purchase_amount_max'].values - \\\n",
        "                                                    new_merchant_feats['new_transac_purchase_amount_min'].values\n",
        "\n",
        "    #create basic agg features from the purchase amount column grouped by card id\n",
        "    op = ['sum', 'mean', 'max', 'min', 'median', 'std', 'skew']\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=op, prefix='hist_transac_', \\\n",
        "                      col='purchase_amount', grpby='card_id')\n",
        "    #finding the difference between the purchase amount max and min\n",
        "    historical_trans_features['hist_transac_amount_diff'] = historical_trans_features['hist_transac_purchase_amount_max'].values - \\\n",
        "                                            historical_trans_features['hist_transac_purchase_amount_min'].values\n",
        "\n",
        "    #basic month features\n",
        "    new_merchant_feats = get_monthlag_stat(new_merchant_feats, new, grpby=['card_id','month_lag'], op='sum',\\\n",
        "                                            col='purchase_amount', prefix='new_transac_', \\\n",
        "                                            name=['1_amount','2_amount'])\n",
        "    # dividing monthlag2 by 1 to find the ratio\n",
        "    new_merchant_feats['new_transac_monthlag_ratio'] = (new_merchant_feats.iloc[:, -1] / new_merchant_feats.iloc[:, -2])\\\n",
        "                                                              .replace([np.inf, -np.inf], np.nan)\n",
        "    #create another feature by taking the log of the ratio\n",
        "    new_merchant_feats['new_transac_monthlag_log_ratio'] = np.log2(new_merchant_feats.iloc[:, -1])\n",
        "\n",
        "    #successive agg features\n",
        "    #create a temp DF ADD to hold the new features\n",
        "    add = successive_aggregates(ht, field1='category_1', field2='purchase_amount')\n",
        "    col = ['installments', 'city_id', 'merchant_category_id', 'merchant_id',\\\n",
        "          'subsector_id','category_2','category_3']\n",
        "\n",
        "    #for each column commpute the agg and merge with the temp DF\n",
        "    for c in col:\n",
        "      add = add.merge(successive_aggregates(ht, c, 'purchase_amount'), \\\n",
        "                on=['card_id'], how='left')\n",
        "    #merge the temp DF with our feature set\n",
        "    new_merchant_feats = new_merchant_feats.merge(add, on=['card_id'], how='left')\n",
        "\n",
        "    #successive agg features\n",
        "    #create a temp DF ADD to hold the new features\n",
        "    add = successive_aggregates(new, 'category_1', 'purchase_amount')\n",
        "    col = ['installments', 'city_id', 'merchant_category_id', 'merchant_id',\\\n",
        "          'subsector_id','category_2','category_3']\n",
        "\n",
        "    #for each column commpute the agg and merge with the temp DF\n",
        "    for c in col:\n",
        "      add = add.merge(successive_aggregates(new, c, 'purchase_amount'), \\\n",
        "                on=['card_id'], how='left')\n",
        "    #merge the temp DF with our feature set\n",
        "    historical_trans_features = historical_trans_features.merge(add, on=['card_id'], how='left')\n",
        "\n",
        "    #save the created features\n",
        "    new_merchant_feats.to_csv('tnew_merch_amount_fillna.csv', index=False)\n",
        "    historical_trans_features.to_csv('thist_transac_amount_fill_na.csv', index=False)\n",
        "    print('Completed FE of purchase amount...')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWe4I-flLc8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fe_tm():\n",
        "  if os.path.isfile('/content/drive/My Drive/case study/upload 15mis/new_merch_time_fill_na.csv'):\n",
        "    print('Completed FE of purchase amount...')\n",
        "  \n",
        "  else:\n",
        "    new = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merchant_processed_fill_na.csv')\n",
        "    new_merchant_feats = pd.DataFrame(new.groupby(['card_id']).size()).reset_index();gc.collect()\n",
        "    new_merchant_feats.columns = ['card_id', 'new_transac_count']\n",
        "\n",
        "    ht = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/ht_processed_fill_na.csv')\n",
        "    historical_trans_features = pd.DataFrame(ht.groupby(['card_id']).size()).reset_index();gc.collect()\n",
        "    historical_trans_features.columns = ['card_id', 'hist_transac_count']\n",
        "    ht['purchase_amount'] = np.round(ht['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
        "\n",
        "    #agg feat like mean, std, max for the column monthlag grouped by card_id \n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max'], prefix='new_transac_', grpby='card_id', col='month_lag')\n",
        "\n",
        "    #get agg feats like min, mean, std for the col specified\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, ['nunique', 'mean', 'std', 'min', 'skew'], 'hist_transac_', 'card_id', 'month_lag')\n",
        "\n",
        "    #get values like min and max values from the col purchase_date\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['purchase_date'], grpby='card_id', op=['max','min'], prefix='new_transac', use_col=True)\n",
        "    #based on the min and max find difference and ratio\n",
        "    new_merchant_feats['purchase_date_diff'] = (pd.to_datetime(new_merchant_feats.iloc[:, -2]) - \n",
        "                                                pd.to_datetime(new_merchant_feats.iloc[:, -1])).dt.days.values\n",
        "    new_merchant_feats['purchase_count_ratio'] = new_merchant_feats['new_transac_count'].values / (1. + new_merchant_feats.iloc[:, -1].values)\n",
        "\n",
        "    #get values like min and max values from the col purchase_date\n",
        "    historical_trans_features = find_single_val(historical_trans_features, ht, col=['purchase_date'], grpby='card_id',\\\n",
        "                                op=['max','min'], prefix='hist_transac', use_col=True)\n",
        "    #create feats like difference and ratio between the first and last purchases made for a card_id\n",
        "    historical_trans_features['hist_purchase_date_diff'] = (pd.to_datetime(historical_trans_features.iloc[:, -2]) - \\\n",
        "                                                            pd.to_datetime(historical_trans_features.iloc[:, -1])).dt.days.values\n",
        "    historical_trans_features['hist_purchase_count_ratio'] = historical_trans_features['hist_transac_count'].values / (1. + historical_trans_features.iloc[:, -1].values)\n",
        "\n",
        "    reference_date = '2018-12-31'\n",
        "    #features based on if the particular day is a weekend\n",
        "    new['is_weekend'] = (pd.DatetimeIndex(new['purchase_date']).dayofweek)\n",
        "    #>5 to check whether the day is sat or sunday if it is then assign a val 1 else 0\n",
        "    new['is_weekend'] = new['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "    #get the values of mean and sum grouped by card_id for the weekend feature\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['is_weekend'], grpby='card_id', name='purchase_weekend_count',\\\n",
        "                                op=['sum'], prefix='new_transac')\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['is_weekend'], grpby='card_id', name='purchase_weekend_mean',\\\n",
        "                                op=['mean'], prefix='new_transac')\n",
        "\n",
        "    #features based on if the particular day is a weekend\n",
        "    #day is termed as weekend if it is either sat or sunday\n",
        "    ht['is_weekend'] = (pd.DatetimeIndex(ht['purchase_date']).dayofweek)\n",
        "    #>5 to check whether the day is sat or sunday if it is then assign a val 1 else 0\n",
        "    ht['is_weekend'] = ht['is_weekend'].apply(lambda x: 1 if x >= 5 else 0).values\n",
        "    #get the values of mean and sum grouped by card_id for the weekend feature\n",
        "    # find purchases made in weekend sum\n",
        "    historical_trans_features = find_single_val(historical_trans_features, ht, col=['is_weekend'], grpby='card_id', \\\n",
        "                                                name='purchase_weekend_count', op=['sum'], prefix='hist_transac')\n",
        "    #find purchases made in weekend mean\n",
        "    historical_trans_features = find_single_val(historical_trans_features, ht, col=['is_weekend'], grpby='card_id', \\\n",
        "                                                name='purchase_weekend_mean', op=['mean'], prefix='hist_transac')\n",
        "    historical_trans_features = historical_trans_features.merge(ht[['card_id', 'reference_month']]\\\n",
        "    .drop_duplicates(), on='card_id', how='left')\n",
        "    historical_trans_features['reference_month'] = pd.to_datetime(historical_trans_features['reference_month'])\n",
        "\n",
        "    purchase_date = pd.to_datetime(new['purchase_date'])\n",
        "    reference_date = pd.to_datetime(reference_date)\n",
        "    # We need to find the difference in days then we can divide by 30 to convert it into months.\n",
        "    # as timedelta doesn't have attribute to directly get months.\n",
        "    new['month_diff'] = (reference_date - purchase_date).dt.days\n",
        "    new['month_diff'] = new['month_diff'] // 30 + new['month_lag']\n",
        "    new['month_diff'].head()\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['month_diff'], grpby='card_id', \\\n",
        "                                        name='new_month_diff_mean', op=['mean'])\n",
        "\n",
        "    purchase_date = pd.to_datetime(ht['purchase_date'])\n",
        "    reference_date = pd.to_datetime(reference_date)\n",
        "    # We need to find the difference in days then we can divide by 30 to convert it into months.\n",
        "    # as timedelta doesn't have attribute to directly get months.\n",
        "    ht['month_diff'] = (reference_date - purchase_date).dt.days\n",
        "    ht['month_diff'] = ht['month_diff'] // 30 + ht['month_lag']\n",
        "    ht['month_diff'].head()\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['mean', 'std', 'min', 'max'], \\\n",
        "                      col='month_diff', grpby='card_id', prefix='hist_')\n",
        "    \n",
        "    new['amount_month_ratio'] = new['purchase_amount'].values / (1. + new['month_diff'].values)\n",
        "    #agg feat based on the cols created in the last part\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'min', 'max', 'skew'], \\\n",
        "                              prefix='new_transac_', grpby='card_id', col='duration')\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'min', 'max', 'skew'], \\\n",
        "                              prefix='new_transac_', grpby='card_id', col='amount_month_ratio')\n",
        "    #find sum and mean of the col monthlag col grouped by card_id\n",
        "    new_merchant_feats = find_single_val(new_merchant_feats, new, col=['month_lag=1', 'month_lag=2'], grpby='card_id',\\\n",
        "                                op=['sum','mean'], prefix='new_transac', use_col=True)\n",
        "\n",
        "    ht['amount_month_ratio'] = ht['purchase_amount'].values / (1. + ht['month_diff'].values)\n",
        "    #agg feat based on the cols created in the last part\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, ['mean', 'std', 'min', 'max', 'skew'], \\\n",
        "                      prefix='hist_transac_', grpby='card_id', col='duration')\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, ['mean', 'std', 'min', 'max', 'skew'], \\\n",
        "                      prefix='hist_transac_', grpby='card_id', col='amount_month_ratio')\n",
        "    #find sum and mean of the col monthlag col grouped by card_id\n",
        "    historical_trans_features = find_single_val(historical_trans_features, ht, col=['month_lag=0', 'month_lag=-1', 'month_lag=-2'],\\\n",
        "                                                grpby='card_id', op=['sum','mean'], prefix='hist_transac', use_col=True)\n",
        "    \n",
        "    #extract week, day, and hour from the date column then\n",
        "    #create agg features like mean, min, max for each of the\n",
        "    #features separately\n",
        "    ht['week'] = pd.DatetimeIndex(ht['purchase_date']).week.values\n",
        "    ht['day'] = pd.DatetimeIndex(ht['purchase_date']).dayofweek.values\n",
        "    ht['hour'] = pd.DatetimeIndex(ht['purchase_date']).hour.values\n",
        "    #get aggregate values from the cols week, day and hour\n",
        "    gc.collect()\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['nunique', 'mean', 'min', 'max'], \\\n",
        "                      col='week', grpby='card_id', prefix='hist_transac')\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['nunique', 'mean', 'min', 'max'], \\\n",
        "                      col='day', grpby='card_id', prefix='hist_transac')\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['nunique', 'mean', 'min', 'max'], \\\n",
        "                      col='hour', grpby='card_id', prefix='hist_transac')\n",
        "    \n",
        "    #calculating the ratio between the two monthlags\n",
        "    new_merchant_feats['new_transac_month_lag=1_2_ratio'] = new_merchant_feats['new_transac_month_lag=1_sum'].values \\\n",
        "                                                            / (1. + new_merchant_feats['new_transac_month_lag=2_sum'].values)\n",
        "    #get basic time feat and create agg features based on the created cols\n",
        "    new = get_basic_time_feat(new, 'card_id', 'purchase_date', 2)\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_1_seconds')\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_1_days')\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_1_hours')\n",
        "    #get basic time feat and create agg features based on the created cols\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_2_seconds')\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_2_days')\n",
        "    new_merchant_feats = s_agg(new_merchant_feats, new, op=['mean', 'std', 'max', 'min'], prefix='new_transac_', \\\n",
        "                              grpby='card_id', col='purchase_date_diff_2_hours')\n",
        "    \n",
        "    #find the ratio between the monthlag cols\n",
        "    historical_trans_features['hist_transac_monthlag_0_-1_ratio'] = historical_trans_features.iloc[:, -6].values \\\n",
        "                                                                  / (1. + historical_trans_features.iloc[:, -4].values)\n",
        "    historical_trans_features['hist_transac_monthlag_0_-2_ratio'] = historical_trans_features.iloc[:, -7].values \\\n",
        "                                                                / (1. + historical_trans_features.iloc[:, -3].values)\n",
        "    #create a feature of the sum of all the three monthlag sum\n",
        "    #crete a temp dataframe which holds the three cols\n",
        "    col = ['hist_transac_month_lag=0_sum', 'hist_transac_month_lag=-1_sum', 'hist_transac_month_lag=-2_sum']\n",
        "    tmp = historical_trans_features[col]\n",
        "    #perform sum operation over the cols\n",
        "    historical_trans_features['hist_transac_3mon_sum'] = tmp.sum(axis=1)\n",
        "    del tmp;gc.collect()\n",
        "    historical_trans_features['hist_transac_3mon_ratio'] = historical_trans_features.iloc[:, -1].values \\\n",
        "                                                          / (1. + historical_trans_features['hist_transac_count'].values)\n",
        "\n",
        "    #if it gives an error use ht['purchase_date'] = pd.to_datetime(ht['purchase_date'])\n",
        "    ht = ht.sort_values('purchase_date')\n",
        "    #get basic time feat and create agg features based on the created cols\n",
        "    ht = get_basic_time_feat(ht, 'card_id', 'purchase_date', 1)\n",
        "    #get basic time feat and create agg features based on the created cols\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['mean', 'std', 'max', 'min'], prefix='hist_transac_', \\\n",
        "                                      grpby='card_id', col='purchase_date_diff_1_seconds')\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['mean', 'std', 'max', 'min'], prefix='hist_transac_', \\\n",
        "                                      grpby='card_id', col='purchase_date_diff_1_days')\n",
        "    historical_trans_features = s_agg(historical_trans_features, ht, op=['mean', 'std', 'max', 'min'], prefix='hist_transac_', \\\n",
        "                                      grpby='card_id', col='purchase_date_diff_1_hours')\n",
        "    \n",
        "    #create influential day features. If a purchase is made withing 100 days\n",
        "    #before or after a festival then it is called as influential days.\n",
        "    holiday = ['ChristmasDay_2017', 'FathersDay_2017', 'ChildrenDay_2017', 'BlackFriday_2017', 'ValentineDay_2017', 'MothersDay_2018']\n",
        "    date = ['2017-12-25', '2017-08-13', '2017-10-12', '2017-11-24', '2017-06-12', '2018-05-13']\n",
        "\n",
        "    for idx, day in enumerate(holiday):\n",
        "      new = get_influential(new, day, date[idx])\n",
        "\n",
        "    #loop through all the created features and add it to the DataFrame\n",
        "    for c in holiday:\n",
        "        gc.collect()\n",
        "        new_merchant_feats['new_transac_{}_mean'.format(c)] = new.groupby(['card_id'])[c]\\\n",
        "                                                              .mean().values\n",
        "    new_merchant_feats.drop(['new_transac_count'], axis=1, inplace=True)\n",
        "\n",
        "    #create influential day features. If a purchase is made withing 100 days\n",
        "    #before or after a festival then it is called as influential days.\n",
        "    ht['purchase_date'] = pd.to_datetime(ht['purchase_date'])\n",
        "    for idx, day in enumerate(holiday):\n",
        "      ht = get_influential(ht, day, date[idx])\n",
        "\n",
        "    #loop through all the created features and add it to the DataFrame\n",
        "    for c in holiday:\n",
        "      gc.collect()\n",
        "      historical_trans_features['hist_transac_{}_mean'.format(c)] = ht.groupby(['card_id'])[c]\\\n",
        "                                                                    .mean().values\n",
        "    historical_trans_features.drop(['hist_transac_count'],axis=1,inplace=True)\n",
        "\n",
        "    #save the created features\n",
        "    new_merchant_feats.to_csv('tnew_merch_time_fillna.csv', index=False)\n",
        "    historical_trans_features.to_csv('thist_transac_time_fill_na.csv', index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DupSzRhGs81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train(train):\n",
        "  print('Started Preprocessing...')\n",
        "  preprocess()\n",
        "  print('Started Feature Engineering...')\n",
        "  fe_inf()\n",
        "  fe_am()\n",
        "  fe_tm()\n",
        "  print('Feature Engineering Completed')\n",
        "\n",
        "  print('Preparing train and test set...')\n",
        "  hist_transac_amount = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/hist_transac_amount_fill_na.csv')\n",
        "  hist_transac_info = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/hist_transac_info_fill_na.csv')\n",
        "  hist_transac_time = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/hist_transac_time_fill_na.csv')\n",
        "  new_merch_amount = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merch_amount_fillna.csv')\n",
        "  new_merch_info = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merch_info_fillna.csv')\n",
        "  new_merch_time = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/new_merch_time_fill_na.csv')\n",
        "\n",
        "  hist_transac_info.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "  new_merch_info.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "  new_merch_time.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "  hist_feats = hist_transac_info.merge(hist_transac_amount, on='card_id', how='left')\n",
        "  hist_feats = hist_feats.merge(hist_transac_time, on='card_id', how='left')\n",
        "  del hist_transac_info, hist_transac_amount, hist_transac_time;gc.collect()\n",
        "\n",
        "  new_feats = new_merch_info.merge(new_merch_amount, on='card_id', how='left')\n",
        "  new_feats = new_feats.merge(new_merch_time, on='card_id', how='left')\n",
        "  del new_merch_info, new_merch_amount, new_merch_time;gc.collect()\n",
        "  print('Loading train and test')\n",
        "  \n",
        "  train_df = train\n",
        "  print('merge train and new features...')\n",
        "  train_df = train_df.merge(hist_feats, on=['card_id'], how='left')\n",
        "  train_df = train_df.merge(new_feats, on=['card_id'], how='left')\n",
        "  train_df['outliers'] = 0\n",
        "  train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
        "  act_date = pd.to_datetime('2018-12-31')\n",
        "  \n",
        "  for df in [train_df]:\n",
        "      #converting the col ref_month an first_act_month to datetime type\n",
        "      reference_month = pd.to_datetime(df['reference_month'])\n",
        "      first_act_month = pd.to_datetime(df['first_active_month'])\n",
        "      #extracting the year and month from the first_act_month\n",
        "      df['year'] = pd.DatetimeIndex(df['first_active_month']).year.values\n",
        "      df['month'] = pd.DatetimeIndex(df['first_active_month']).month.values\n",
        "      df['month_diff'] = (reference_month - \\\n",
        "                          first_act_month).dt.days.values\n",
        "      df['elapsed_days'] = (act_date - reference_month).dt.days.values\n",
        "      df['hist_purchase_active_diff'] = (pd.to_datetime(df['hist_transac_purchase_date_min'].astype(str)\\\n",
        "                                          .apply(lambda x: x[:7])) - first_act_month).dt.days.values\n",
        "      df['hist_purchase_recency'] = (act_date - pd.to_datetime(df['hist_transac_purchase_date_max'])).dt.days.values\n",
        "      df['new_purchase_recency'] = (act_date - pd.to_datetime(df['new_transac_purchase_date_max'])).dt.days.values\n",
        "  \n",
        "  train_cols = [c for c in train_df.columns if c not in ['hist_transac_purchase_date_max', 'hist_transac_purchase_date_min', 'new_transac_purchase_date_max', 'new_transac_purchase_date_min',\\\n",
        "  'hist_purchase_date_last', 'hist_purchase_date_first', 'reference_month', 'hist_purchase_a_date_last', 'hist_purchase_a_date_first', 'new_purchase_date_last', 'new_purchase_date_first','card_id', 'first_active_month','first_active_month', 'target','outliers','feature_1','feature_2','feature_3','refernce_month','ref_first_month_diff_days']]\n",
        "  target = train_df['target']\n",
        "  outliers = train_df['outliers']\n",
        "  card_id = train_df['card_id']\n",
        "  del train_df['target']\n",
        "  print('Completed')\n",
        "  return train_df, train_cols"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWKP7TpEBXlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fun_1(train):\n",
        "  train = train\n",
        "  train_df, train_cols = get_train(train)\n",
        "      \n",
        "  print('Loading pickle file...')\n",
        "  path = '/content/drive/My Drive/case study/upload 15mis/lgb_final_323_tune.sav'\n",
        "  import pickle\n",
        "  with open(path, 'rb') as pickle_file:\n",
        "      mod = pickle.load(pickle_file)\n",
        "  predictions = mod.predict(train_df[train_cols])\n",
        "  return predictions"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INKxoXZyZT_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/train.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Q97zSnnviM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_fun_2(train, target):\n",
        "  train = train\n",
        "  target = target\n",
        "  predictions = fun_1(train=train)\n",
        "  score = np.sqrt(mean_squared_error(predictions, target))\n",
        "  print('Actual Value:', target)\n",
        "  print('Predicted Value:', predictions)\n",
        "  print('RMSE Score:', score)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KKDCLEvaoAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "9b797a16-7f00-4969-fa27-2844f879244f"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "final_fun_2(train=train_df, target=train_df['target'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Preprocessing...\n",
            "Preprocessing New Merchant Dataset...\n",
            "Filled Missing values for new_merchant...\n",
            "Working on Historical Transaction Dataset...\n",
            "Filled missing values for historical transaction dataset...\n",
            "Loading Merchant Dataset for preprocessing...\n",
            "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
            "Filled missing values for merchant dataset...\n",
            "One Hot Encoding the variables...\n",
            "Completed...\n",
            "Started Feature Engineering...\n",
            "Completed FE of transaction info...\n",
            "Completed FE of purchase amount...\n",
            "Completed FE of purchase amount...\n",
            "Feature Engineering Completed\n",
            "Preparing train and test set...\n",
            "Loading train and test\n",
            "merge train and new features...\n",
            "Completed\n",
            "Loading pickle file...\n",
            "RMSE Score: 3.6535296284973575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXCtTQbfjw1F",
        "colab_type": "text"
      },
      "source": [
        "**Model on a random single point from the train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS_phnH2iBqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "ef7bde26-82da-480d-ebcd-329c4a70356c"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/case study/upload 15mis/train.csv')\n",
        "train_df = data.sample(1)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "final_fun_2(train=train_df, target=train_df['target'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Preprocessing...\n",
            "Preprocessing New Merchant Dataset...\n",
            "Filled Missing values for new_merchant...\n",
            "Working on Historical Transaction Dataset...\n",
            "Filled missing values for historical transaction dataset...\n",
            "Loading Merchant Dataset for preprocessing...\n",
            "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
            "Filled missing values for merchant dataset...\n",
            "One Hot Encoding the variables...\n",
            "Completed...\n",
            "Started Feature Engineering...\n",
            "Completed FE of transaction info...\n",
            "Completed FE of purchase amount...\n",
            "Completed FE of purchase amount...\n",
            "Feature Engineering Completed\n",
            "Preparing train and test set...\n",
            "Loading train and test\n",
            "merge train and new features...\n",
            "Completed\n",
            "Loading pickle file...\n",
            "Actual Value: 140372   -0.0333\n",
            "Name: target, dtype: float64\n",
            "Predicted Value: [-0.33539999]\n",
            "RMSE Score: 0.3021004233466764\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}